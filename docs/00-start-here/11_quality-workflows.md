# Quality Workflows Framework

**Version:** 3.7.0+
**Last Updated:** 2025-12-15
**Status:** Planned (Coming after v3.6.0)

---

## Overview

The **Quality Workflows Framework** addresses the critical gap between AI code generation speed and code quality, breaking through the productivity glass ceiling that teams hit when they scale AI-assisted development.

**Inspiration:** [The State of AI Code Quality: Hype vs Reality ‚Äî Itamar Friedman, Qodo](https://www.youtube.com/watch?v=rgjF5o2Qjsc)

**Core Problem:**
- 67% of developers have serious quality concerns about AI-generated code
- AI code generation = 97% more PRs, 90% more review time
- 3x increase in security incidents correlates with 3x increase in code written
- **Productivity glass ceiling:** Speed gains lost to fixing quality issues

**Result:**
- Automated quality gates at every SDLC phase
- Testing doubles trust in AI-generated code
- 47% improvement in code review productivity
- 66% reduction in security incidents (projected)
- Breaking through to **2-3x sustained productivity**

---

## The Quality Crisis in AI Code Generation

### Current State of AI-Assisted Development

**High Adoption:**
- 60% of developers report 25%+ of code is AI-generated
- 15% report 80%+ of code is AI-generated
- Some teams report 3x productivity boost in writing code

**The Quality Problem:**

```
The Productivity Paradox:

Speed: 3x faster code generation
‚Üì
Quality Issues: 67% have serious concerns
‚Üì
Consequences:
  - 97% more PRs (workload explosion)
  - 90% more review time per PR
  - 42% more dev time fixing issues
  - 35% project delays
  - 3x security incidents
‚Üì
Net Result: Productivity glass ceiling at ~1.2x
```

**The "Vibe Coding" Trap:**
- Generate code fast without rigor
- "Vibe checking" and "vibe reviewing" (gut feel, not systematic)
- Standards only "mostly followed" not rigorously enforced
- Quality issues discovered late (expensive to fix)

### Breaking the Glass Ceiling

**Qodo's Core Thesis:**
> "To break the productivity glass ceiling and achieve 2x or 10x gains, companies must invest in agentic quality workflows that operate throughout the Software Development Life Cycle."

**Quality workflows are not overhead** - they're the **enabler** of AI productivity:

```
Without Quality (Current):
‚îú‚îÄ Speed: 3x faster generation
‚îú‚îÄ Quality: 3x more security incidents
‚îî‚îÄ Net: Glass ceiling at ~1.2x (speed gains lost)

With Quality (v3.7.0):
‚îú‚îÄ Speed: 3x faster generation
‚îú‚îÄ Quality: Automated gates maintain standards
‚îî‚îÄ Net: 2-3x sustained productivity (gains preserved)
```

---

## Quality-First Architecture

### Automated Quality Gates

**Pattern:** Parallel agents validate code before human review

```
Code Generation ‚Üí Quality Gates (parallel) ‚Üí Human Review
                  ‚Üì
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îú‚îÄ Security Scanner
          ‚îú‚îÄ Test Generator
          ‚îú‚îÄ Standards Enforcer
          ‚îú‚îÄ Performance Analyzer
          ‚îî‚îÄ Documentation Validator
```

**Benefits:**
- **Parallel execution** = Fast feedback (gates don't block each other)
- **Automated first pass** = Humans review higher-level concerns
- **Consistent enforcement** = No "tired reviewer" problem
- **Scalability** = Handles 97% more PRs without 90% more human time

### SDLC Integration

**Quality at Every Phase (Not Just Review):**

| Phase | Quality Agent | What It Checks |
|-------|---------------|----------------|
| **Planning** | Requirements Validator | Testable? Clear? Measurable? |
| **Development** | Lint + Security Scanner | Standards + OWASP top 10 + CVEs |
| **Review** | Automated Code Reviewer | Fundamentals checklist + patterns |
| **Testing** | Test Generator + Coverage | features.json tests + % coverage |
| **Deployment** | Security + Smoke Tests | Final validation + deployment sanity |

**Why Every Phase:**
- Catch issues early = Cheaper to fix
- Shift left = Quality built in, not bolted on
- Continuous validation = No "surprise" failures at the end

### Learning Standards (Not Static Rules)

**The Problem with Static Rules:**
- Rules in Copilot/Cursor config are only "mostly followed"
- Teams evolve, rules become stale
- One-size-fits-all doesn't work for team culture

**Dynamic Learning Approach:**

```
Quality Suggestion ‚Üí Developer Feedback ‚Üí Model Update
"Avoid nested if"  ‚Üí Rejected 80%     ‚Üí Lower confidence
"Add null check"   ‚Üí Accepted 95%     ‚Üí Higher confidence
```

**Learning Loop:**
1. Quality agent makes suggestion with confidence score
2. Developer accepts or rejects
3. System tracks acceptance rate over time
4. Adjusts confidence thresholds for future suggestions
5. Adapts to team preferences (e.g., "This team prefers early returns over nested ifs")

**Example:**
```json
// quality-standards.json (living, learning)
{
  "rules": [
    {
      "id": "nested-ifs",
      "suggestion": "Consider early returns instead of nested if statements",
      "confidence": 45,
      "acceptanceRate": 0.20,
      "status": "learning"
    },
    {
      "id": "null-checks",
      "suggestion": "Add null check before dereferencing",
      "confidence": 95,
      "acceptanceRate": 0.95,
      "status": "enforced"
    }
  ]
}
```

### Context Engine for Quality

**Core Insight:** Context is the #1 factor for AI code quality

> "Developers who distrust AI code cited insufficient LLM context 80% of the time."

**What Context Includes:**

**Level 1: Codebase Context**
- Current code files
- Project structure
- Dependencies (package.json, requirements.txt)

**Level 2: Standards Context** (8% of context usage)
- coding-standards.md
- CLAUDE.md (team conventions)
- .eslintrc, .prettierrc (enforced standards)

**Level 3: Historical Context**
- Git history (versioning)
- PR history (what passed review before)
- Commit messages (rationale for decisions)

**Level 4: Quality Context**
- Test results (what's passing/failing)
- Security audit logs
- Performance benchmarks
- Code review feedback patterns

**All context feeds into quality agents** ‚Üí Higher confidence validation

---

## The Quality Agents

### 1. Security Scanner Agent

**Purpose:** Prevent the 3x security incidents problem

**What It Checks:**
- **OWASP Top 10:** SQL injection, XSS, CSRF, etc.
- **Dependency CVEs:** Known vulnerabilities in dependencies
- **Authentication/Authorization:** Proper access controls
- **Data validation:** Input sanitization, output encoding
- **Secrets detection:** API keys, passwords in code

**Example Output:**
```
üîí Security Scan Results

‚ùå CRITICAL: SQL Injection Risk
   File: src/api/users.ts:42
   Issue: Direct string interpolation in SQL query
   Fix: Use parameterized queries

   Bad:  db.query(`SELECT * FROM users WHERE id = ${userId}`)
   Good: db.query('SELECT * FROM users WHERE id = ?', [userId])

   Confidence: 98%
   Why: Pattern matches known SQL injection vulnerability
   Impact: User data could be exposed or modified

‚úÖ No XSS vulnerabilities detected (10 checks passed)
‚úÖ CSRF protection properly implemented
‚ö†Ô∏è  Medium: API key in .env.example (likely false positive)

Overall: 2 issues found, 1 critical requires immediate fix
```

### 2. Test Generator Agent

**Purpose:** Double trust in AI code through testing

> "Developers who use AI to test their code double their trust in AI-generated code."

**What It Does:**
- Reads features.json tasks
- Generates tests for each feature based on test criteria
- Ensures coverage thresholds met
- Updates features.json with test file paths

**Integration with v3.6.0 Domain Memory:**
```json
// features.json (from v3.6.0)
{
  "features": [
    {
      "id": 1,
      "title": "User model with email/password",
      "status": "failing",
      "tests": ["models/user.test.ts passes"],
      "dependencies": []
    }
  ]
}

// Test-generator creates:
// models/user.test.ts with:
//   - Email validation tests
//   - Password hashing tests
//   - Required field tests
//   - Edge case handling
```

**Example Output:**
```
üß™ Test Generation Results

Feature #1: User model with email/password

Generated: models/user.test.ts (8 tests)
  ‚úì Should require email field
  ‚úì Should require password field
  ‚úì Should validate email format
  ‚úì Should reject invalid emails
  ‚úì Should hash password before saving
  ‚úì Should not expose password in JSON
  ‚úì Should handle duplicate emails
  ‚úì Should validate password strength

Coverage: 95% (exceeds 80% threshold)

Confidence: 92%
Why: All edge cases covered, follows testing best practices

Next: Run tests with `npm test` to verify implementation
```

### 3. Standards Enforcer Agent

**Purpose:** Ensure code follows team conventions (dynamically learned)

**What It Checks:**
- Coding style (indentation, naming conventions)
- Documentation standards (JSDoc, type annotations)
- Error handling patterns
- Performance anti-patterns
- Team-specific conventions (from coding-standards.md)

**Learning Behavior:**
```
Week 1: Suggests 20 style fixes, 50% accepted
Week 2: Learns team preferences, adjusts confidence
Week 3: Suggests 15 style fixes, 80% accepted (better targeting)
Week 4: Primarily enforces high-confidence rules, rarely rejected
```

**Example Output:**
```
üìã Standards Check Results

‚úÖ Code Style: 95% adherence
   - Indentation: Consistent 2-space
   - Naming: camelCase for variables, PascalCase for classes

‚ö†Ô∏è  Documentation: 60% coverage (below 80% threshold)
   - Missing JSDoc: 8 functions
   - Missing type annotations: 3 parameters

   Suggestion (Confidence: 75%):
   Add JSDoc to exported functions for better maintainability

   Would you like me to generate JSDoc for these functions?
   [This suggestion has 75% acceptance rate on your team]

‚úÖ Error Handling: Proper try/catch blocks
‚ùå Performance: Nested loop detected (O(n¬≤) complexity)

   File: src/utils/matcher.ts:28
   Issue: Could use Set for O(n) lookup instead

   Suggestion (Confidence: 88%):
   Convert inner array to Set for better performance
```

### 4. Quality Reviewer Agent

**Purpose:** Automated code review before human review

**What It Reviews:**
- Fundamentals checklist (security, performance, testing, error handling)
- Code patterns and idioms
- Maintainability concerns
- Documentation completeness
- Integration with existing code

**Integrates with v3.5.0 Jewels Patterns:**
- Confidence scores (95% confident this needs fixing)
- Rationale provision (WHY this is an issue)
- JIT cheat sheets (HOW to fix it)

**Example Output:**
```
üîç Code Review Results

Feature #2: Password hashing with bcrypt

Overall Assessment: Needs Minor Changes Before Merge

Security: ‚úÖ Excellent (100% confidence)
  - bcrypt properly configured
  - Salt rounds appropriate (12)
  - No plaintext password storage

Performance: ‚ö†Ô∏è Acceptable (70% confidence)
  - Blocking operation on main thread
  - Consider async bcrypt.hash() instead of hashSync()
  - Impact: May slow down request handling under load

Testing: ‚ùå Insufficient (95% confidence - HIGH)
  - No test for password verification
  - No test for invalid salt rounds
  - Missing: Should fail on weak passwords

  Why this matters:
  Password hashing is security-critical. Without tests,
  future refactoring could introduce vulnerabilities.

  Quick fix (JIT cheat sheet):
  Add these tests to auth/password.test.ts:

  test('should verify correct password', async () => {
    const hash = await hashPassword('test123');
    expect(await verifyPassword('test123', hash)).toBe(true);
  });

Documentation: ‚úÖ Good
  - Clear function signatures
  - JSDoc describes behavior

Recommendation: Add missing tests, then approve
Confidence: 90% (This pattern has 92% agreement rate)
```

---

## Integration with Existing Framework

### With v3.5.0 (Jewels Proactive Intelligence)

**Quality gates use Jewels patterns:**

**Confidence Scores:**
```
Security issue detected (Confidence: 98%) ‚Üí HIGH priority
Style suggestion (Confidence: 45%) ‚Üí LOW priority, learning
```

**Rationale Provision:**
```
‚ùå Issue: SQL injection risk
Why it matters: User data could be exposed or modified
How to fix: Use parameterized queries (example provided)
```

**JIT Cheat Sheets:**
```
‚ö†Ô∏è  Missing tests for Feature #3

üìù Quick Reference:

Testing Pattern for API Endpoints:
  test('GET /users should return 200', async () => {
    const res = await request(app).get('/users');
    expect(res.status).toBe(200);
  });

Detailed guide: docs/02-optimization/TESTING_PATTERNS.md
```

### With v3.6.0 (Anthropic Domain Memory)

**Quality integrated into Coder Agent workflow:**

**Before (Without Quality):**
```
Coder Agent:
1. Pick feature from features.json
2. Build it incrementally
3. Run tests
4. Mark as "passing"
5. Commit
```

**After (With Quality):**
```
Coder Agent:
1. Pick feature from features.json
2. Build it incrementally
3. Run tests
4. ‚≠ê Quality gates (parallel):
   ‚îú‚îÄ Security scan
   ‚îú‚îÄ Test coverage check
   ‚îú‚îÄ Standards enforcement
   ‚îî‚îÄ Performance analysis
5. IF all gates pass ‚Üí Mark as "passing"
6. IF any gates fail ‚Üí Fix issues, retry
7. Commit with quality verification
```

**Quality prevents premature "passing" status:**
```json
{
  "features": [
    {
      "id": 2,
      "title": "Password hashing",
      "status": "failing",
      "qualityGates": {
        "security": "passed",
        "tests": "failed",
        "standards": "passed",
        "performance": "warning"
      }
    }
  ]
}
```

**Feature only marked "passing" when ALL quality gates pass.**

---

## Before & After Comparison

### Scenario: Building Authentication System (15 Features)

**Without Quality Workflows (v3.6.0 Only):**

```
Session 1-3: Build features fast with domain memory
  - features.json tracks progress (8/15 complete)
  - Bootup ritual works (instant regrounding)
  - Atomic commits (clean git history)

Session 4: User testing reveals:
  ‚ùå 3 SQL injection vulnerabilities (not caught)
  ‚ùå 5 features missing tests (not enforced)
  ‚ùå 2 performance issues (not detected)

  Result: 42% more dev time fixing issues
          35% project delay
          Trust in AI code: LOW (60%)

Total: 10 sessions (3 spent fixing quality issues)
```

**With Quality Workflows (v3.6.0 + v3.7.0):**

```
Session 1-3: Build features with quality gates
  - features.json tracks progress (8/15 complete)
  - Quality gates catch issues immediately:
    ‚úì Security scan blocks 3 vulnerable implementations
    ‚úì Test generator ensures coverage before "passing"
    ‚úì Performance analyzer flags slow operations

  - Fix issues as you go (cheap, not expensive later)
  - Quality badges visible in features.json

Session 4: User testing reveals:
  ‚úÖ No security issues (caught by scanner)
  ‚úÖ All features tested (enforced by gates)
  ‚úÖ Performance within acceptable range

  Result: Smooth deployment
          No delay
          Trust in AI code: HIGH (95%)

Total: 7 sessions (3 sessions saved by preventing rework)

Difference:
  - 30% fewer sessions (quality issues prevented)
  - 66% fewer security incidents (gates caught them)
  - 2x trust in AI code (testing + validation)
  - Sustainable productivity (no glass ceiling)
```

---

## Implementation Roadmap

### Phase 1: Quality Agents (Weeks 1-2)

**Create 3 quality agents:**

1. **`.claude/agents/security-scanner.md`**
   - OWASP top 10 checks
   - CVE scanning for dependencies
   - Secrets detection

2. **`.claude/agents/test-generator.md`**
   - Reads features.json tasks
   - Generates tests based on criteria
   - Ensures coverage thresholds

3. **`.claude/agents/quality-reviewer.md`**
   - Fundamentals checklist automation
   - Pattern detection
   - Integration validation

### Phase 2: Quality Infrastructure (Weeks 2-3)

**Build supporting infrastructure:**

4. **`scripts/quality-gate.sh`**
   - Pre-commit hook integration
   - Parallel gate execution
   - Configurable gate selection
   - Blocks commit if gates fail

5. **`.claude/quality-standards.json`**
   - Living standards file
   - Tracks acceptance/rejection rates
   - Adapts confidence thresholds
   - Team-personalized rules

6. **`scripts/quality-dashboard.sh`**
   - Visual metrics display
   - "95% pass security checks"
   - Trend analysis over time
   - Trust-building statistics

### Phase 3: SDLC Integration (Week 3)

**Integrate quality at every phase:**

7. **Planning Phase:**
   - Requirements validator (testable? clear?)
   - Acceptance criteria checker

8. **Development Phase:**
   - Lint + security scan during build
   - Real-time feedback in terminal

9. **Review Phase:**
   - Automated review before human review
   - Quality badges on features

10. **Testing Phase:**
    - Test generation + coverage enforcement
    - Quality gate verification

11. **Deployment Phase:**
    - Final security scan
    - Smoke tests
    - Deployment readiness check

### Phase 4: Learning & Adaptation (Ongoing)

**Continuous improvement:**

12. **Feedback Loop:**
    - Track suggestion acceptance/rejection
    - Update quality-standards.json automatically
    - Adjust confidence scores based on history
    - Personalize to team style

13. **Metrics Collection:**
    - Trust in AI code (before/after surveys)
    - Review time per PR (measure 90% ‚Üí reduction)
    - Security incidents (measure 66% reduction)
    - Quality concerns (measure 67% ‚Üí 20%)

---

## Success Metrics (Projected)

**Critical Disclaimer:** Following template's honesty policy, these are PROJECTED metrics based on Qodo's research and industry data. Template has 1 user (author). We need real-world validation.

### Quantitative (Industry Data from Qodo)

1. **Trust in AI Code**
   - Current: 67% have serious quality concerns
   - Projected: 2x trust increase (testing effect)
   - Target: Quality concerns 67% ‚Üí 20%

2. **Review Productivity**
   - Current: 90% more review time per PR
   - Projected: 47% improvement (automated first pass)
   - Target: Review time 90% ‚Üí 40% overhead

3. **Security**
   - Current: 3x security incidents
   - Projected: 66% reduction (gates catch vulnerabilities)
   - Target: Return to baseline incident rate

4. **Development Time**
   - Current: 42% more time fixing quality issues
   - Projected: 75% reduction (catch early = cheaper fix)
   - Target: Fix time 42% ‚Üí 10% overhead

5. **Project Delays**
   - Current: 35% delays due to quality issues
   - Projected: 70% reduction
   - Target: Delays 35% ‚Üí 10%

### Qualitative (Projected)

6. **Confidence in Shipping**
   - Before: "Did I catch everything?" anxiety
   - After: "Quality gates verified" confidence
   - Measurement: Developer surveys

7. **Learning Velocity**
   - Before: Learn through painful bugs in production
   - After: Learn from good/bad examples + quality feedback
   - Measurement: 3x faster learning (estimated)

8. **Sustainable Productivity**
   - Before: Glass ceiling at ~1.2x (speed lost to quality)
   - After: Breaking through to 2-3x sustained productivity
   - Measurement: Long-term velocity tracking

### Validation Request

**We need YOUR data!**
- Try quality workflows when v3.7.0 releases
- Report actual trust levels, review time, security incidents
- Submit feedback: [GitHub issues](https://github.com/christianearle01/claude-config-template/issues)
- Help us replace "Projected" with "Validated" metrics

---

## Best Practices

### DO Use Quality Workflows For:
- ‚úÖ Production code (quality is critical)
- ‚úÖ Long-running projects (quality compounds over time)
- ‚úÖ Team projects (consistent standards across developers)
- ‚úÖ Security-sensitive code (automated scanning prevents incidents)
- ‚úÖ Projects with AI-generated code (67% have concerns without quality)

### DON'T Use Quality Workflows For:
- ‚ùå Quick prototypes/experiments (overhead not worth it)
- ‚ùå Throwaway code (quality doesn't matter if it's temporary)
- ‚ùå Single-file scripts (manual review is faster)
- ‚ùå Personal learning projects (unless learning quality practices)

### Progressive Adoption:

**Beginner (v3.4.0):**
- Manual verification (Ctrl+O)
- Fundamentals checklist
- Code review docs

**Intermediate (v3.5.0):**
- Proactive intelligence
- Confidence scores
- JIT cheat sheets

**Advanced (v3.6.0):**
- Domain memory
- Persistent state
- Bootup ritual

**Expert (v3.7.0):**
- Quality workflows
- Automated gates
- SDLC integration

Each level builds on previous, not replacing but enhancing.

---

## Troubleshooting

### "Quality gates are too strict / too many false positives"

**Solution:** Adjust confidence thresholds in quality-standards.json

```json
{
  "securityGate": {
    "blockOnConfidence": 90,  // Only block on high confidence (default)
    "warnOnConfidence": 60    // Warn on medium confidence
  }
}
```

### "Quality gates slow down development"

**Problem:** Gates running sequentially instead of parallel
**Solution:** Ensure quality-gate.sh runs gates in parallel:

```bash
# Bad (sequential):
run_security_scan && run_tests && run_standards

# Good (parallel):
run_security_scan &
run_tests &
run_standards_check &
wait  # All complete before returning
```

### "Gates disagree with team standards"

**Expected:** Learning phase (first 2-4 weeks)
**Solution:** Let system learn from your feedback:
- Accept suggestions that match team style
- Reject suggestions that don't
- System will adapt confidence scores over time

After 4 weeks, acceptance rate should be 80%+

### "Quality gates missed an issue"

**Action:**
1. Report the issue (helps improve gates)
2. Add specific check to quality-standards.json
3. Adjust confidence threshold if needed

**Example:**
```json
{
  "customChecks": [
    {
      "id": "our-api-pattern",
      "check": "All API routes must have rate limiting",
      "pattern": "app.(get|post|put|delete)\\(",
      "requires": "rateLimiter",
      "confidence": 100
    }
  ]
}
```

---

## FAQ

### "Isn't quality checking what code review is for?"

**Both are important, different purposes:**

- **Automated quality gates**: Catch mechanical issues (security, tests, standards)
- **Human code review**: Judge architecture, design decisions, maintainability

Gates **assist** human review, not replace:
- Automated first pass (catches 70-80% of issues)
- Humans focus on high-level concerns (20-30%)
- Result: 47% more productive human review

### "Will quality gates slow me down?"

**Short term:** Maybe 5-10% slower (gates take time to run)
**Long term:** 2-3x faster (prevent expensive rework)

**Math:**
- Without gates: Fast generation, 42% time fixing issues = Net 1.2x
- With gates: 10% slower generation, 10% time fixing = Net 2.4x

**Quality gates SAVE time** by shifting issues left (catch early = cheaper).

### "How is this different from linters/CI checks?"

**Quality workflows include traditional tools but go further:**

**Traditional Linters/CI:**
- Static analysis (syntax, style)
- Run once (on commit or PR)
- Generic rules (not team-specific)
- Binary (pass/fail)

**Quality Workflows (v3.7.0):**
- Dynamic learning (adapts to team)
- SDLC integration (every phase, not just commit)
- Context-aware (uses codebase + standards + history)
- Confidence-scored (95% confident = enforce, 45% = suggest)
- AI-generated test suggestions

Quality workflows **augment** linters, not replace them.

### "Can I use this without v3.5.0 or v3.6.0?"

**Yes, but synergy is powerful:**

**Standalone v3.7.0:**
- Quality gates work independently
- Enforce standards, security, testing
- Value: Catch issues, build trust

**v3.5.0 + v3.7.0:**
- Quality gates use confidence scores (Jewels)
- Provide rationale (educational)
- JIT cheat sheets (self-service fixes)
- Value: Quality + learning

**v3.6.0 + v3.7.0:**
- Quality gates validate features.json tasks
- Feature only marked "passing" when quality verified
- Quality badges in domain memory
- Value: Quality + persistence

**v3.5.0 + v3.6.0 + v3.7.0:**
- Complete AI-native toolkit
- Intelligence + Persistence + Quality
- Value: Breaking productivity glass ceiling

Each version standalone provides value, but together they're exponentially more powerful.

---

## Related Resources

**Within Template:**
- `do../00-start-here/05_anti-patterns.md` - Vibe coding vs AI augmented
- `docs/00-start-here/10_proactive-paradigm.md` - Jewels proactive intelligence
- `CLAUDE.md` - Domain memory architecture
- Plan: `/Users/christianearle01/.claude/plans/nifty-weaving-raven.md` - Full roadmap

**External:**
- [Qodo (formerly Codium)](https://www.qodo.ai/) - AI code quality platform
- [OWASP Top 10](https://owasp.org/www-project-top-ten/) - Security vulnerabilities
- [Test-Driven Development](https://martinfowler.com/bliki/TestDrivenDevelopment.html) - Testing best practices

---

## Complete AI-Native Toolkit

Quality Workflows (v3.7.0) completes the four-layer framework:

| Layer | Framework | Purpose | Problem Solved |
|-------|-----------|---------|----------------|
| **1. Intelligence** | Jewels (v3.5.0) | Proactive suggestions | Decision fatigue, context switching |
| **2. Persistence** | Anthropic (v3.6.0) | Domain memory | "Where were we?", regrounding cost |
| **3. Quality** | Qodo (v3.7.0) | Automated gates | Trust crisis, productivity glass ceiling |
| **4. Orchestration** | Collective (v4.0.0) | Multi-project | Cross-project inconsistencies |

**Together:**
- Intelligence: HOW to suggest (confidence, rationale)
- Persistence: WHAT to persist (features.json, bootup ritual)
- Quality: HOW to validate (gates, tests, security)
- Orchestration: WHEN to coordinate (team patterns)

**Result:** Sustainable 2-3x productivity with confidence

---

**Remember:** Quality workflows are not overhead preventing speed - they're the **foundation** that makes AI-assisted speed sustainable. Without quality, fast code generation hits a glass ceiling. With quality, you break through to genuine 2-3x productivity gains that last.
