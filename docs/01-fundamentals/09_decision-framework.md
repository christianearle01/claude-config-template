# Decision Framework: Which Tool for Which Task?

**Version:** v4.18.0
**Updated:** 2025-12-17

Master the art of tool selection - knowing WHEN to use WHAT and WHY.

---

## Table of Contents

1. [Why Decision Frameworks Matter](#why-decision-frameworks-matter)
2. [Tool Selection Matrix](#tool-selection-matrix)
3. [Complexity Flowchart](#complexity-flowchart)
4. [Agents vs Commands vs Skills](#agents-vs-commands-vs-skills)
5. [Model Selection Integration](#model-selection-integration)
6. [Decision Trees by Task Type](#decision-trees-by-task-type)
7. [Quick Reference](#quick-reference)

---

## Why Decision Frameworks Matter

### The Problem: Integration Paralysis

After learning individual tools (agents, commands, skills, MCPs), users face a new challenge:
- "I know what each tool does, but which one should I use NOW?"
- "Should I use an agent or just read the file directly?"
- "When is sequential-thinking worth the extra tokens?"

**This is integration paralysis** - having powerful tools but not knowing which to choose.

### The Solution: Decision Frameworks

A decision framework is a **structured approach to tool selection** that:
1. Reduces cognitive load (follow the framework, not gut feeling)
2. Builds expertise faster (learn the patterns, not trial-and-error)
3. Optimizes outcomes (right tool = better results, fewer tokens)

**This guide teaches you the decision-making process, not just the tools.**

---

## Tool Selection Matrix

Quick reference: Match your task type to the recommended tool.

| Task Type | Recommended Tool | Why | Token Cost |
|-----------|------------------|-----|------------|
| **Read known file** | Read tool | Direct, fast, minimal overhead | Low (100-500) |
| **Find files by pattern** | Glob tool | Optimized for pattern matching | Low (50-200) |
| **Search code for keyword** | Grep tool | Fast text search | Low (100-300) |
| **Explore unknown codebase** | Explore agent | Systematic discovery | Medium (1K-3K) |
| **Plan implementation** | Plan agent | Architecture thinking | Medium (2K-5K) |
| **Complex decision** | Adversarial-validator agent | Multi-perspective analysis | High (3K-8K) |
| **Optimize prompt** | Prompt-polisher agent | Structured optimization | Medium (500-2K) |
| **Choose workflow mode** | Mode-selector agent | 6-dimension scoring | Medium (1K-2K) |
| **Repeated query** | Skill (auto-invoked) | Context-aware answers | Low (250-600) |
| **Workflow shortcut** | Slash command | Pre-configured sequence | Low (varies) |
| **Multi-step reasoning** | Sequential-thinking MCP | Structured analysis | Medium (1K-3K) |

**Key principle:** Start with the simplest tool that solves the problem. Escalate only when needed.

---

## Complexity Flowchart

### When to Escalate from Simple to Complex Tools

```
START: What's the task?

â”œâ”€ KNOWN file location?
â”‚  YES â†’ Use Read tool directly âœ“
â”‚  NO â†’ Continue to exploration
â”‚
â”œâ”€ Simple PATTERN search?
â”‚  YES â†’ Use Glob (files) or Grep (content) âœ“
â”‚  NO â†’ Continue to exploration
â”‚
â”œâ”€ Need to UNDERSTAND codebase structure?
â”‚  YES â†’ Use Explore agent âœ“
â”‚  NO â†’ Continue to decision complexity
â”‚
â”œâ”€ Multiple VALID approaches?
â”‚  YES â†’ Use Adversarial-validator agent âœ“
â”‚  NO â†’ Continue to implementation
â”‚
â”œâ”€ Need to PLAN multi-file changes?
â”‚  YES â†’ Use Plan agent âœ“
â”‚  NO â†’ Use direct tools (Read + Edit)
â”‚
â””â”€ IMPLEMENT changes
   â†’ Use appropriate tools (Edit, Write, Bash)
```

**Escalation triggers:**
1. **Uncertainty** - Don't know where to start â†’ Explore agent
2. **Complexity** - Multiple files, unclear scope â†’ Plan agent
3. **High stakes** - Architecture decisions â†’ Adversarial-validator
4. **Optimization** - Want better approach â†’ Mode-selector + Prompt-polisher

---

## Agents vs Commands vs Skills

### Understanding the Three Tool Types

| Feature | Agents | Commands | Skills |
|---------|--------|----------|--------|
| **Invocation** | `@agent-name` | `/command` | Automatic |
| **Purpose** | Complex tasks | Workflow shortcuts | Knowledge queries |
| **Scope** | READ+WRITE | READ+WRITE | READ-only |
| **Token cost** | High (1K-8K) | Varies | Low (250-600) |
| **Best for** | Multi-step tasks | Repeated operations | Frequent questions |
| **Examples** | Plan, Explore | /build, /test, /review | Projects-registry |

### When to Use Each

**Use Agents when:**
- Task requires multi-step reasoning
- Need structured output or analysis
- Want confidence scores and breakdowns
- Exploring multiple approaches
- **Example:** "I need to plan a refactoring strategy" â†’ Plan agent

**Use Commands when:**
- Running project-specific workflows
- Repeated operations (build, test, review)
- Shortcuts for common sequences
- **Example:** "Run the test suite" â†’ /test command

**Use Skills when:**
- Asking questions about known topics
- Querying project registry or status
- Need quick answers without exploration
- **Example:** "Which projects need updates?" â†’ Projects-registry skill auto-invokes

### Combining Tools

**Powerful patterns:**
1. **Skill â†’ Agent:** Skill answers question, agent acts on it
   - "Show outdated projects" (skill) â†’ Plan updates (agent)

2. **Agent â†’ Command:** Agent plans, command executes
   - Plan refactoring (agent) â†’ /review to validate (command)

3. **Command â†’ Skill:** Command runs, skill provides insights
   - /test (command) â†’ Testing-workflow skill diagnoses failures

---

## Model Selection Integration

### Three-Question Method (from v3.11.0)

After choosing your tool, choose your model:

```
1. Is this CREATIVE or ARCHITECTURAL work?
   YES â†’ Use Opus
   Examples: System design, architecture decisions, complex planning

2. Does this involve WRITING CODE?
   YES â†’ Use Sonnet
   Examples: Implementation, refactoring, bug fixes

3. Is this MECHANICAL or EXPLORATORY?
   YES â†’ Use Haiku
   Examples: File search, simple reads, pattern matching
```

### Model Selection by Tool

**Pre-configured in agents** (`.claude/agents/*.md`):

| Agent | Recommended Model | Why |
|-------|-------------------|-----|
| Adversarial-validator | **Opus** | Multi-perspective reasoning, high-stakes decisions |
| Project-planner | **Opus** | 6-perspective architecture analysis |
| Initializer | **Opus** | Feature decomposition, learning path design |
| Coder | **Sonnet** | Balanced implementation work |
| Prompt-polisher | **Sonnet** | Structured prompt optimization |
| Explore | **Haiku** | Fast codebase navigation |
| Quality-reviewer | **Haiku** | Rule-based orchestration |

**Cost impact:**
- Explore with Haiku vs Sonnet: 91% cheaper
- Adversarial-validator with Opus vs Sonnet: Worth 3x cost for better decisions
- Overall: 24% token reduction when using optimal models

---

## Decision Trees by Task Type

### Bug Fix Decision Tree

```
1. Do you know which FILE has the bug?
   YES â†’ Read the file directly
   NO â†’ Use Grep to search error message or Explore agent

2. Is the ROOT CAUSE obvious?
   YES â†’ Fix directly with Edit
   NO â†’ Use sequential-thinking MCP to analyze

3. Will the fix affect MULTIPLE files?
   YES â†’ Use Plan agent to coordinate changes
   NO â†’ Fix the single file

4. Is this a CRITICAL bug in production?
   YES â†’ Use Testing-workflow skill + Quality-reviewer agent
   NO â†’ Standard fix â†’ test â†’ commit workflow
```

**Example:**
- Known bug in `auth.ts` line 45 â†’ Read â†’ Edit â†’ Done (200 tokens)
- Mystery authentication error â†’ Grep "auth error" â†’ Read files â†’ sequential-thinking â†’ Plan â†’ Fix (3K tokens, but thorough)

---

### New Feature Decision Tree

```
1. Is the feature WELL-DEFINED with clear requirements?
   YES â†’ Use Plan agent â†’ Implement
   NO â†’ Use Project-planner agent first (6 perspectives)

2. Will this require MULTIPLE sessions to complete?
   YES â†’ Use Domain Memory pattern (Initializer + Coder agents)
   NO â†’ Standard implementation

3. Does this involve ARCHITECTURE decisions?
   YES â†’ Use Adversarial-validator agent (3 personas)
   NO â†’ Proceed with Plan agent

4. Do you need QUALITY validation?
   YES â†’ Use Quality-reviewer agent after implementation
   NO â†’ Standard review process
```

**Example:**
- Add logout button â†’ Plan â†’ Implement â†’ Done (2K tokens)
- Build user dashboard â†’ Project-planner (Opus) â†’ Initializer â†’ Coder (Sonnet) â†’ Quality-reviewer (4 sessions, 12K tokens total, but robust)

---

### Refactoring Decision Tree

```
1. Is the refactoring scope CLEAR?
   YES â†’ Plan agent â†’ Implement
   NO â†’ Explore agent first to understand structure

2. Are there MULTIPLE valid approaches?
   YES â†’ Adversarial-validator agent (compare approaches)
   NO â†’ Proceed with chosen approach

3. Will this affect MANY files?
   YES â†’ Create detailed plan, phase the work
   NO â†’ Direct refactoring

4. Need to maintain BACKWARD compatibility?
   YES â†’ Quality-reviewer agent for validation
   NO â†’ Standard testing
```

**Example:**
- Extract 3 functions to utils â†’ Read â†’ Edit â†’ Done (500 tokens)
- Refactor authentication system â†’ Explore â†’ Adversarial-validator (3 approaches) â†’ Plan â†’ Implement in phases (8K tokens, but prevents rework)

---

### Optimization Decision Tree

```
1. What are you optimizing?
   CODE PERFORMANCE â†’ Profile first, Plan changes
   TOKEN USAGE â†’ Mode-selector + Prompt-polisher agents
   WORKFLOW SPEED â†’ Analyze bottlenecks, use appropriate automation

2. Do you know the OPTIMAL approach?
   YES â†’ Implement directly
   NO â†’ Use Adversarial-validator to compare approaches

3. Is this a ONE-TIME optimization?
   YES â†’ Do it manually, document the pattern
   NO â†’ Create slash command or skill for reuse

4. Need to measure IMPACT?
   YES â†’ Before/after metrics, validate improvement
   NO â†’ Implement and monitor
```

**Example:**
- "My prompts are too verbose" â†’ Prompt-polisher (educational mode) â†’ Learn patterns â†’ Apply (1.5K tokens, learn for future)
- "Which workflow mode?" â†’ Mode-selector agent â†’ Get recommendation (1K tokens)

---

## Quick Reference

### Start Here: 3-Step Decision Process

**Step 1: Define the task**
- What am I trying to accomplish?
- What do I already know vs need to discover?

**Step 2: Choose the simplest tool**
- Known file â†’ Read
- Unknown location â†’ Glob or Grep
- Need exploration â†’ Explore agent
- Complex decision â†’ Higher-level agent

**Step 3: Escalate only if needed**
- Simple tool didn't work? â†’ Try next level
- Multiple approaches possible? â†’ Use decision agent
- High stakes? â†’ Use validation agent

---

### Red Flags: When You're Using the Wrong Tool

**ðŸš© Over-using agents for simple tasks**
- "I need to read config.json" â†’ Don't use Explore agent, just Read
- **Fix:** Use simplest tool first (Read), escalate only if needed

**ðŸš© Using sequential-thinking for trivial decisions**
- "Should I use const or let?" â†’ Don't need deep analysis
- **Fix:** Trust your judgment on straightforward choices

**ðŸš© Skipping exploration phase**
- Editing files without understanding codebase structure
- **Fix:** Use Explore agent first, especially in unfamiliar code

**ðŸš© Not using skills for repeated questions**
- Manually checking project status every time
- **Fix:** Skills auto-invoke for common queries (e.g., projects-registry)

**ðŸš© Using wrong model**
- Running Explore agent with Opus (expensive, unnecessary)
- **Fix:** Match model to task complexity (Haiku for exploration)

---

### Decision Framework Cheat Sheet

**Print or bookmark this:**

```
TASK                          â†’ TOOL                    â†’ MODEL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Read known file               â†’ Read                    â†’ N/A
Find files                    â†’ Glob                    â†’ N/A
Search code                   â†’ Grep                    â†’ N/A
Explore codebase              â†’ Explore agent           â†’ Haiku
Plan implementation           â†’ Plan agent              â†’ Sonnet
Architecture decision         â†’ Adversarial-validator   â†’ Opus
Optimize prompt               â†’ Prompt-polisher         â†’ Sonnet
Choose workflow               â†’ Mode-selector           â†’ Sonnet
Complex reasoning             â†’ Sequential-thinking MCP â†’ Sonnet
Quick status check            â†’ Skill (auto)            â†’ N/A
Project workflow              â†’ Slash command           â†’ N/A
```

---

## Related Resources

**Learn more about the tools mentioned:**
- [Agents](#custom-agents) - 7 specialized agents with confidence scoring
- [Skills](02_skills-paradigm.md) - Automatic expertise invocation
- [Model Selection](../02-optimization/01_model-selection-strategy.md) - Three-Question Method
- [Prompt Patterns](08_prompt-patterns.md) - 10 reusable templates
- [Integration Patterns](../02-optimization/06_integration-patterns.md) - How tools work together (coming in v4.18.0)

**See it in action:**
- [Workflow Examples](../../examples/workflows/) - 4 end-to-end scenarios (coming in v4.18.0)

---

## Success Criteria

You've mastered the decision framework when you can:

1. âœ… **Quickly choose** the right tool for a task (< 30 seconds decision time)
2. âœ… **Explain your reasoning** (why this tool, not that one)
3. âœ… **Escalate appropriately** (start simple, add complexity only when needed)
4. âœ… **Avoid over-engineering** (not using Opus when Haiku suffices)
5. âœ… **Combine tools effectively** (agents + commands + skills working together)

**Next steps:**
1. Read through the [Workflow Examples](../../examples/workflows/) to see decisions in context
2. Practice with your own tasks, consciously applying the decision trees
3. Review your tool usage after a week - are you using the optimal tools?

---

**Keywords:** decision framework, tool selection, agents, commands, skills, complexity, escalation, optimization, workflow, meta-learning

